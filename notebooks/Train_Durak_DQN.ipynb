{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Train Durak DQN.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "HIPp7PI2hgsC",
        "ATCK9BHeb8tg",
        "6hktZgpTftP2"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "gFyj5ob_K4X7",
        "outputId": "9b2b178a-bfb8-42d7-e3b6-95d4cc4fe7b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive\n",
            "mkdir: cannot create directory ‘Github’: File exists\n",
            "/content/drive/MyDrive/Github\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/Github'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "git_token='tok'\n",
        "username='maxmarsakov'\n",
        "repository='durak-project'\n",
        "%cd '/content/drive/MyDrive/'\n",
        "%mkdir 'Github'\n",
        "base_folder='/content/drive/MyDrive/Github'\n",
        "%cd {base_folder}\n",
        "%pwd"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run this once: the First time - **clone**"
      ],
      "metadata": {
        "id": "PfSYHNNCsnJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://{git_token}@github.com/{username}/{repository}\n"
      ],
      "metadata": {
        "id": "xosgmZiINHap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Second Time pull\n",
        "**Important**!!! after this command all local changes will be gone, so be sure to save unsaved work"
      ],
      "metadata": {
        "id": "wekp5vXkspG_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd {base_folder}/{repository}\n",
        "!git reset --hard HEAD\n",
        "!git pull"
      ],
      "metadata": {
        "id": "LXJPK4NOsYB3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb421960-344b-4a3c-9355-9ddcae82fc97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Github/durak-project\n",
            "Checking out files: 100% (54/54), done.\n",
            "HEAD is now at 195768d probabalistic threshold\n",
            "remote: Enumerating objects: 11, done.\u001b[K\n",
            "remote: Counting objects: 100% (11/11), done.\u001b[K\n",
            "remote: Compressing objects: 100% (4/4), done.\u001b[K\n",
            "remote: Total 6 (delta 2), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (6/6), done.\n",
            "From https://github.com/maxmarsakov/durak-project\n",
            "   195768d..c7b46f3  master     -> origin/master\n",
            "Updating 195768d..c7b46f3\n",
            "Fast-forward\n",
            " experiments/dmc_result/durak/model.tar | Bin \u001b[31m19020181\u001b[m -> \u001b[32m19020181\u001b[m bytes\n",
            " 1 file changed, 0 insertions(+), 0 deletions(-)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## run this"
      ],
      "metadata": {
        "id": "ObCBZIzpgKKb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd {base_folder}/{repository}\n",
        "!pip install -r requirements.txt\n",
        "%cd {base_folder}/{repository}\n",
        "%env PROJECT_PATH={base_folder}/{repository}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V5Uz1ZMDRBKv",
        "outputId": "f1bd08ad-e5e8-4eb8-8f1c-45ab5a5ca9a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Github/durak-project\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting colorama\n",
            "  Downloading colorama-0.4.5-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 2)) (1.21.6)\n",
            "Collecting rlcard[torch]\n",
            "  Downloading rlcard-1.0.7.tar.gz (268 kB)\n",
            "\u001b[K     |████████████████████████████████| 268 kB 9.5 MB/s \n",
            "\u001b[?25hCollecting translation\n",
            "  Downloading translation-1.0.5-py3-none-any.whl (9.3 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from translation->-r requirements.txt (line 4)) (2.23.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->translation->-r requirements.txt (line 4)) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->translation->-r requirements.txt (line 4)) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->translation->-r requirements.txt (line 4)) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->translation->-r requirements.txt (line 4)) (2022.6.15)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.7/dist-packages (from rlcard[torch]->-r requirements.txt (line 3)) (1.1.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from rlcard[torch]->-r requirements.txt (line 3)) (1.12.1+cu113)\n",
            "Collecting GitPython\n",
            "  Downloading GitPython-3.1.27-py3-none-any.whl (181 kB)\n",
            "\u001b[K     |████████████████████████████████| 181 kB 49.6 MB/s \n",
            "\u001b[?25hCollecting gitdb2\n",
            "  Downloading gitdb2-4.0.2-py3-none-any.whl (1.1 kB)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from rlcard[torch]->-r requirements.txt (line 3)) (3.2.2)\n",
            "Collecting gitdb>=4.0.1\n",
            "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 2.1 MB/s \n",
            "\u001b[?25hCollecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython->rlcard[torch]->-r requirements.txt (line 3)) (4.1.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->rlcard[torch]->-r requirements.txt (line 3)) (1.4.4)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->rlcard[torch]->-r requirements.txt (line 3)) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->rlcard[torch]->-r requirements.txt (line 3)) (3.0.9)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->rlcard[torch]->-r requirements.txt (line 3)) (0.11.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->rlcard[torch]->-r requirements.txt (line 3)) (1.15.0)\n",
            "Building wheels for collected packages: rlcard\n",
            "  Building wheel for rlcard (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rlcard: filename=rlcard-1.0.7-py3-none-any.whl size=325373 sha256=5a7d6ebd571ee68eba77f7b01c975a3825c4f29b572a395920582ed255bf5bda\n",
            "  Stored in directory: /root/.cache/pip/wheels/8a/90/bd/bc402a48ca90970c9a7c2c4387dcb885fdf6073ec231a605ad\n",
            "Successfully built rlcard\n",
            "Installing collected packages: smmap, gitdb, rlcard, GitPython, gitdb2, translation, colorama\n",
            "Successfully installed GitPython-3.1.27 colorama-0.4.5 gitdb-4.0.9 gitdb2-4.0.2 rlcard-1.0.7 smmap-5.0.0 translation-1.0.5\n",
            "/content/drive/MyDrive/Github/durak-project\n",
            "env: PROJECT_PATH=/content/drive/MyDrive/Github/durak-project\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "import torch\n",
        "import time\n",
        "sys.path.insert(0,os.environ['PROJECT_PATH'])\n",
        "sys.path.insert(0,os.environ['PROJECT_PATH']+\"/durak_rlcard\")\n",
        "\n",
        "import rlcard\n",
        "from rlcard.agents import RandomAgent\n",
        "from rlcard.utils import (\n",
        "    get_device,\n",
        "    set_seed,\n",
        "    tournament,\n",
        "    reorganize,\n",
        "    Logger,\n",
        "    plot_curve,\n",
        ")\n",
        "from durak_rlcard.env import DurakEnv\n",
        "from durak_rlcard.agents import SimpleAgent, SimpleLearningAgent, SimpleProbaAgent\n",
        "import random\n",
        "from collections import namedtuple\n",
        "CustomArgs=namedtuple('CustomArgs',['agent','opponent','cuda','seed','num_episodes','num_eval_games','evaluate_every','save_every','log_dir'])"
      ],
      "metadata": {
        "id": "kkW2P5YHihUS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(args,env,agent,evaluate_vs=None):\n",
        "\n",
        "    # Start training\n",
        "    curr_time=None\n",
        "    with Logger(args.log_dir) as logger:\n",
        "        for episode in range(args.num_episodes):\n",
        "\n",
        "            if args.agent == 'nfsp':\n",
        "                agent.sample_episode_policy()\n",
        "\n",
        "            # Generate data from the environment\n",
        "            trajectories, payoffs = env.run(is_training=True)\n",
        "\n",
        "            # Reorganaize the data to be state, action, reward, next_state, done\n",
        "            trajectories = reorganize(trajectories, payoffs)\n",
        "\n",
        "            # Feed transitions into agent memory, and train the agent\n",
        "            # Here, we assume that DQN always plays the first position\n",
        "            # and the other players play randomly (if any)\n",
        "            for ts in trajectories[0]:\n",
        "                agent.feed(ts)\n",
        "\n",
        "            if args.opponent==\"self\":\n",
        "                # if playing against self, copy self as opponent\n",
        "                env.set_agents([agent,agent])\n",
        "\n",
        "            # Evaluate the performance. Play with random agents.\n",
        "            if episode % args.evaluate_every == 0:\n",
        "                prev_agents=env.agents\n",
        "                if evaluate_vs is not None:\n",
        "                    env.set_agents([agent,evaluate_vs])\n",
        "                logger.log_performance(\n",
        "                    env.timestep,\n",
        "                    tournament(\n",
        "                        env,\n",
        "                        args.num_eval_games,\n",
        "                    )[0]\n",
        "                )\n",
        "                if evaluate_vs is not None:\n",
        "                    env.set_agents(prev_agents)\n",
        "            \n",
        "            if curr_time is None or ( (time.perf_counter()-curr_time) > 60 * args.save_every):\n",
        "                # as well save the model\n",
        "                save_path = os.path.join(args.log_dir, 'model.pth')\n",
        "                torch.save(agent, save_path)\n",
        "                curr_time=time.perf_counter()\n",
        "\n",
        "        # Get the paths\n",
        "        csv_path, fig_path = logger.csv_path, logger.fig_path\n",
        "\n",
        "    # Plot the learning curve\n",
        "    plot_curve(csv_path, fig_path, args.agent)\n",
        "\n",
        "    # Save model\n",
        "    save_path = os.path.join(args.log_dir, 'model.pth')\n",
        "    torch.save(agent, save_path)\n",
        "    print('Model saved in', save_path)"
      ],
      "metadata": {
        "id": "QFbxDiO9hfuz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_agent_opponent(env,args):\n",
        "    agent,opponent=None,None\n",
        "\n",
        "    if args.agent == 'dqn':\n",
        "        from rlcard.agents import DQNAgent\n",
        "        agent = DQNAgent(\n",
        "            num_actions=env.num_actions,\n",
        "            state_shape=env.state_shape[0],\n",
        "            mlp_layers=[64,64],\n",
        "            device=device,\n",
        "        )\n",
        "    elif args.agent == 'nfsp':\n",
        "        from rlcard.agents import NFSPAgent\n",
        "        agent = NFSPAgent(\n",
        "            num_actions=env.num_actions,\n",
        "            state_shape=env.state_shape[0],\n",
        "            hidden_layers_sizes=[64,64],\n",
        "            q_mlp_layers=[64,64],\n",
        "            device=device,\n",
        "        )\n",
        "    elif args.agent == 'simple_learning':\n",
        "        from agents import SimpleLearningAgent\n",
        "        agent = SimpleLearningAgent(\n",
        "            num_actions=env.num_actions,\n",
        "            state_shape=env.state_shape[0],\n",
        "            mlp_layers=[64,64],\n",
        "            device=device,\n",
        "        )\n",
        "    elif args.agent == 'simple_proba':\n",
        "        # simple callback is needed to determine \n",
        "        # when to use simple vs dqn strategy\n",
        "        def pcallback(state):\n",
        "            raw=state['raw_obs']\n",
        "            deckSize=raw['deckSize']\n",
        "            # set these hyperparameters\n",
        "            endCardsSize=10\n",
        "            probaStart=0.2\n",
        "            probaEnd=0.8\n",
        "            if deckSize<=endCardsSize: \n",
        "                # endgame\n",
        "                return 'dqn' if random.random() < probaEnd else 'simple'\n",
        "            # start game\n",
        "            return 'dqn' if random.random() < probaStart else 'simple'\n",
        "\n",
        "        from agents import SimpleProbaAgent\n",
        "        agent = SimpleProbaAgent(\n",
        "            num_actions=env.num_actions,\n",
        "            state_shape=env.state_shape[0],\n",
        "            mlp_layers=[64,64],\n",
        "            device=device,\n",
        "            use_strategy_callback=pcallback\n",
        "        )\n",
        "    \n",
        "    # set opponent\n",
        "    if args.opponent=='random':\n",
        "        from rlcard.agents import RandomAgent\n",
        "        opponent=RandomAgent(num_actions=env.num_actions)\n",
        "    elif args.opponent=='simple':\n",
        "        from agents import SimpleAgent\n",
        "        opponent=SimpleAgent(num_actions=env.num_actions)\n",
        "    elif args.opponent=='self':\n",
        "        # copy agent\n",
        "        opponent=agent\n",
        "\n",
        "    return tuple([agent,opponent])"
      ],
      "metadata": {
        "id": "9XX3vjekevmM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train DQN on gpu\n"
      ],
      "metadata": {
        "id": "V3CnF9q3QZ0k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!python3 durak_rlcard/dqn.py --cuda=0"
      ],
      "metadata": {
        "id": "yeyfm7iJR-WM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DQN vs Random"
      ],
      "metadata": {
        "id": "HIPp7PI2hgsC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#['agent','opponent','cuda','seed','num_episodes','num_eval_games','evaluate_every','save_every','log_dir']\n",
        "args=CustomArgs(\"dqn\",\"random\",\"0\",42,10000,2000,100,10,\"experiments/dqn_vs_random\")\n",
        "# Check whether gpu is available\n",
        "device = get_device()\n",
        "set_seed(args.seed)\n",
        "env = DurakEnv()\n",
        "agent,opponent=get_agent_opponent(env,args)\n",
        "# Initialize the agent and use random agents as opponents\n",
        "agents = [agent,opponent]\n",
        "env.set_agents(agents)\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.cuda\n",
        "train(args,env,agent)"
      ],
      "metadata": {
        "id": "0-IKAvqiboOV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DQN vs Simple"
      ],
      "metadata": {
        "id": "yrR_biiqbL0b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#['agent','opponent','cuda','seed','num_episodes','num_eval_games','evaluate_every','save_every','log_dir']\n",
        "args=CustomArgs(\"dqn\",\"simple\",\"0\",42,5000,2000,100,15,\"experiments/dqn_vs_simple\")\n",
        "# Check whether gpu is available\n",
        "device = get_device()\n",
        "set_seed(args.seed)\n",
        "env = DurakEnv()\n",
        "agent,opponent=get_agent_opponent(env,args)\n",
        "# Initialize the agent and use random agents as opponents\n",
        "agents = [agent,opponent]\n",
        "env.set_agents(agents)\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.cuda\n",
        "train(args,env,agent)"
      ],
      "metadata": {
        "id": "jFtKMS3CiubG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Simple learning vs Simple"
      ],
      "metadata": {
        "id": "ATCK9BHeb8tg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#['agent','opponent','cuda','seed','num_episodes','num_eval_games','evaluate_every','save_every','log_dir']\n",
        "args=CustomArgs(\"simple_learning\",\"simple\",\"0\",42,5000,2000,100,30,\"experiments/simple_learning_vs_simple_deep\")\n",
        "# Check whether gpu is available\n",
        "device = get_device()\n",
        "set_seed(args.seed)\n",
        "env = DurakEnv()\n",
        "agent,opponent=get_agent_opponent(env,args)\n",
        "agent = SimpleLearningAgent(\n",
        "    num_actions=env.num_actions,\n",
        "    state_shape=env.state_shape[0],\n",
        "    mlp_layers=[64,64],\n",
        "    device=device,\n",
        ")\n",
        "# Initialize the agent and use random agents as opponents\n",
        "agents = [agent,opponent]\n",
        "env.set_agents(agents)\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.cuda\n",
        "train(args,env,agent)"
      ],
      "metadata": {
        "id": "bOs5dXcrcCWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Simple learning vs SELF"
      ],
      "metadata": {
        "id": "ZOMXCQg_cYKo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#['agent','opponent','cuda','seed','num_episodes','num_eval_games','evaluate_every','save_every','log_dir']\n",
        "args=CustomArgs(\"simple_learning\",\"self\",\"0\",42,10000,2000,100,15,\"experiments/simple_learning_vs_self_new_reward\")\n",
        "# Check whether gpu is available\n",
        "device = get_device()\n",
        "set_seed(args.seed)\n",
        "env = DurakEnv()\n",
        "agent,opponent=get_agent_opponent(env,args)\n",
        "agent = SimpleLearningAgent(\n",
        "    num_actions=env.num_actions,\n",
        "    state_shape=env.state_shape[0],\n",
        "    mlp_layers=[64,64,64],\n",
        "    device=device,\n",
        ")\n",
        "# Initialize the agent and use random agents as opponents\n",
        "agents = [agent,opponent]\n",
        "env.set_agents(agents)\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.cuda\n",
        "train(args,env,agent,evaluate_vs=SimpleAgent(num_actions=env.num_actions))"
      ],
      "metadata": {
        "id": "-8jxWv7zcbDy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Simple learning probabalistic vs Simple"
      ],
      "metadata": {
        "id": "fEjewGErcGnc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#['agent','opponent','cuda','seed','num_episodes','num_eval_games','evaluate_every','save_every','log_dir']\n",
        "args=CustomArgs(\"simple_learning\",\"self\",\"0\",42,10000,1000,100,15,\"experiments/simple_proba_vs_self\")\n",
        "# Check whether gpu is available\n",
        "device = get_device()\n",
        "set_seed(args.seed)\n",
        "env = DurakEnv()\n",
        "agent,opponent=get_agent_opponent(env,args)\n",
        "agent =SimpleProbaAgent(\n",
        "    num_actions=env.num_actions,\n",
        "    state_shape=env.state_shape[0],\n",
        "    mlp_layers=[64,64],\n",
        "    device=device,\n",
        "    proba_at_start=0.1,\n",
        "    proba_at_end=0.9,\n",
        ")\n",
        "# Initialize the agent and use random agents as opponents\n",
        "agents = [agent,opponent]\n",
        "env.set_agents(agents)\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.cuda\n",
        "train(args,env,agent,evaluate_vs=SimpleAgent(num_actions=env.num_actions))\n"
      ],
      "metadata": {
        "id": "WRZmnfWTcFwK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Simple probabalistic with Threshold vs Simple"
      ],
      "metadata": {
        "id": "t6mi7nHGBStF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#['agent','opponent','cuda','seed','num_episodes','num_eval_games','evaluate_every','save_every','log_dir']\n",
        "args=CustomArgs(\"simple_learning\",\"self\",\"0\",42,5000,1000,100,15,\"experiments/simple_proba_vs_self_threshold_0.1_0.6_thr_6\")\n",
        "# Check whether gpu is available\n",
        "device = get_device()\n",
        "set_seed(args.seed)\n",
        "env = DurakEnv()\n",
        "agent,opponent=get_agent_opponent(env,args)\n",
        "agent =SimpleProbaAgent(\n",
        "    num_actions=env.num_actions,\n",
        "    state_shape=env.state_shape[0],\n",
        "    mlp_layers=[64,64],\n",
        "    device=device,\n",
        "    proba_at_start=0.1,\n",
        "    proba_at_end=0.6,\n",
        "    threshold=6\n",
        ")\n",
        "# Initialize the agent and use random agents as opponents\n",
        "agents = [agent,opponent]\n",
        "env.set_agents(agents)\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.cuda\n",
        "train(args,env,agent,evaluate_vs=SimpleAgent(num_actions=env.num_actions))"
      ],
      "metadata": {
        "id": "ldhOBs6WBRjF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Deeper"
      ],
      "metadata": {
        "id": "3NFLeuuffi8-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#['agent','opponent','cuda','seed','num_episodes','num_eval_games','evaluate_every','save_every','log_dir']\n",
        "args=CustomArgs(\"simple_learning\",\"self\",\"0\",42,15000,1000,500,15,\"experiments/simple_proba_vs_self_0.1_0.8_256x3_thr6\")\n",
        "# Check whether gpu is available\n",
        "device = get_device()\n",
        "set_seed(args.seed)\n",
        "env = DurakEnv()\n",
        "agent,opponent=get_agent_opponent(env,args)\n",
        "agent =SimpleProbaAgent(\n",
        "    num_actions=env.num_actions,\n",
        "    state_shape=env.state_shape[0],\n",
        "    mlp_layers=[365,365,365],\n",
        "    epsilon_decay_steps=40000,\n",
        "    device=device,\n",
        "    proba_at_start=0.1,\n",
        "    proba_at_end=0.6, # vary -> 0.6\n",
        "    threshold=0\n",
        ")\n",
        "# Initialize the agent and use random agents as opponents\n",
        "agents = [agent,opponent]\n",
        "env.set_agents(agents)\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.cuda\n",
        "train(args,env,agent,evaluate_vs=SimpleAgent(num_actions=env.num_actions))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "CRlqv7dzfmDn",
        "outputId": "68763df9-5844-45bf-a6b2-d0341fcb2fc4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--> Running on the GPU\n",
            "\n",
            "----------------------------------------\n",
            "  timestep     |  55\n",
            "  reward       |  0.391\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rINFO - Step 100, rl-loss: 0.29810842871665955\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 1100, rl-loss: 0.08695834875106812\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 2100, rl-loss: 0.10081396996974945\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 3100, rl-loss: 0.05264813452959061\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 4100, rl-loss: 0.014624183997511864\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 5100, rl-loss: 0.025551527738571167\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 6100, rl-loss: 0.022654615342617035\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 7100, rl-loss: 0.04111135005950928\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 8100, rl-loss: 0.0181155726313591\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 9100, rl-loss: 0.023861585184931755\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 10100, rl-loss: 0.023677267134189606\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 11100, rl-loss: 0.03150387480854988\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 12100, rl-loss: 0.012934105470776558\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 13100, rl-loss: 0.022795215249061584\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 14100, rl-loss: 0.011984926648437977\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 15100, rl-loss: 0.011845957487821579\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 15376, rl-loss: 0.030582310631871223\n",
            "----------------------------------------\n",
            "  timestep     |  90456\n",
            "  reward       |  0.444\n",
            "----------------------------------------\n",
            "INFO - Step 16100, rl-loss: 0.017652273178100586\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 17100, rl-loss: 0.017591888085007668\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 18100, rl-loss: 0.017596233636140823\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 19100, rl-loss: 0.018170740455389023\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 20100, rl-loss: 0.009530262090265751\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 21100, rl-loss: 0.013014747761189938\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 22100, rl-loss: 0.011414248496294022\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 23100, rl-loss: 0.016025980934500694\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 24100, rl-loss: 0.010744508355855942\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 25100, rl-loss: 0.01171463169157505\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 26100, rl-loss: 0.010629046708345413\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 27100, rl-loss: 0.013530919328331947\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 28100, rl-loss: 0.02287295088171959\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 29100, rl-loss: 0.01775561273097992\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 30100, rl-loss: 0.01829763688147068\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 30742, rl-loss: 0.018785499036312103\n",
            "----------------------------------------\n",
            "  timestep     |  180647\n",
            "  reward       |  0.426\n",
            "----------------------------------------\n",
            "INFO - Step 31100, rl-loss: 0.021039053797721863\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 32100, rl-loss: 0.018175847828388214\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 33100, rl-loss: 0.013246283866465092\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 34100, rl-loss: 0.03602491319179535\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 35100, rl-loss: 0.02192622236907482\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 36100, rl-loss: 0.009758159518241882\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 37100, rl-loss: 0.012185078114271164\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 38100, rl-loss: 0.015133397653698921\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 39100, rl-loss: 0.021188601851463318\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 40100, rl-loss: 0.011899465695023537\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 41100, rl-loss: 0.027478445321321487\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 42100, rl-loss: 0.016071096062660217\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 43100, rl-loss: 0.025408199056982994\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 44100, rl-loss: 0.012798525393009186\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 45100, rl-loss: 0.018823252990841866\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 46038, rl-loss: 0.01179128885269165\n",
            "----------------------------------------\n",
            "  timestep     |  270714\n",
            "  reward       |  0.462\n",
            "----------------------------------------\n",
            "INFO - Step 46100, rl-loss: 0.01352973747998476\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 47100, rl-loss: 0.00827151071280241\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 48100, rl-loss: 0.009578880853950977\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 49100, rl-loss: 0.017786137759685516\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 50100, rl-loss: 0.02083488553762436\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 51100, rl-loss: 0.010744319297373295\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 52100, rl-loss: 0.017982402816414833\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 53100, rl-loss: 0.0148607874289155\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 54100, rl-loss: 0.009507456794381142\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 55100, rl-loss: 0.015197963453829288\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 56100, rl-loss: 0.01272241398692131\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 57100, rl-loss: 0.012416301295161247\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 58100, rl-loss: 0.014728730544447899\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 59100, rl-loss: 0.007974188774824142\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 60100, rl-loss: 0.015665140002965927\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 61100, rl-loss: 0.016140246763825417\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 61191, rl-loss: 0.010930167511105537\n",
            "----------------------------------------\n",
            "  timestep     |  360372\n",
            "  reward       |  0.419\n",
            "----------------------------------------\n",
            "INFO - Step 62100, rl-loss: 0.013861940242350101\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 63100, rl-loss: 0.008973808027803898\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 64100, rl-loss: 0.015787458047270775\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 65100, rl-loss: 0.0062732938677072525\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 66100, rl-loss: 0.01959311030805111\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 67100, rl-loss: 0.016173185780644417\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 68100, rl-loss: 0.008557185530662537\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 69100, rl-loss: 0.0076178316958248615\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 70100, rl-loss: 0.0047258855774998665\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 71100, rl-loss: 0.014376530423760414\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 72100, rl-loss: 0.00991770252585411\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 73100, rl-loss: 0.010838700458407402\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 74100, rl-loss: 0.008200732059776783\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 75100, rl-loss: 0.01480390876531601\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 76100, rl-loss: 0.009938865900039673\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 76594, rl-loss: 0.021211666986346245\n",
            "----------------------------------------\n",
            "  timestep     |  450119\n",
            "  reward       |  0.429\n",
            "----------------------------------------\n",
            "INFO - Step 77100, rl-loss: 0.010836743749678135\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 78100, rl-loss: 0.011160949245095253\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 79100, rl-loss: 0.004644409753382206\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 80100, rl-loss: 0.007796111982315779\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 81100, rl-loss: 0.010606508702039719\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 82100, rl-loss: 0.007461260072886944\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 83100, rl-loss: 0.00855138897895813\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 84100, rl-loss: 0.01751149818301201\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 85100, rl-loss: 0.01615048199892044\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 86100, rl-loss: 0.013617035001516342\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 87100, rl-loss: 0.016128530725836754\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 88100, rl-loss: 0.013645021244883537\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 89100, rl-loss: 0.007506855763494968\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 90100, rl-loss: 0.02335551008582115\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 91100, rl-loss: 0.01729857176542282\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 91585, rl-loss: 0.011376154609024525\n",
            "----------------------------------------\n",
            "  timestep     |  539483\n",
            "  reward       |  0.42\n",
            "----------------------------------------\n",
            "INFO - Step 92100, rl-loss: 0.011084726080298424\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 93100, rl-loss: 0.040405984967947006\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 94100, rl-loss: 0.014255478978157043\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 95100, rl-loss: 0.011217435821890831\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 96100, rl-loss: 0.01588641107082367\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 97100, rl-loss: 0.004709383472800255\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 98100, rl-loss: 0.008473176509141922\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 99100, rl-loss: 0.016663387417793274\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 100100, rl-loss: 0.007132712751626968\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 101100, rl-loss: 0.008066273294389248\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 102100, rl-loss: 0.008307626470923424\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 103100, rl-loss: 0.01806519739329815\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 104100, rl-loss: 0.011290136724710464\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 105100, rl-loss: 0.005187814123928547\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 106100, rl-loss: 0.006932383868843317\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 106888, rl-loss: 0.006314537487924099\n",
            "----------------------------------------\n",
            "  timestep     |  630147\n",
            "  reward       |  0.434\n",
            "----------------------------------------\n",
            "INFO - Step 107100, rl-loss: 0.011965667828917503\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 108100, rl-loss: 0.012081526219844818\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 109100, rl-loss: 0.014075110666453838\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 110100, rl-loss: 0.007162942551076412\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 111100, rl-loss: 0.010658791288733482\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 112100, rl-loss: 0.013369869440793991\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 113100, rl-loss: 0.019488513469696045\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 114100, rl-loss: 0.008858054876327515\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 115100, rl-loss: 0.0075507573783397675\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 116100, rl-loss: 0.025700487196445465\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 117100, rl-loss: 0.007275701034814119\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 118100, rl-loss: 0.00915277935564518\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 119100, rl-loss: 0.006305141840130091\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 120100, rl-loss: 0.00819537602365017\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 121100, rl-loss: 0.016267724335193634\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 122100, rl-loss: 0.006150614470243454\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 122171, rl-loss: 0.014317454770207405\n",
            "----------------------------------------\n",
            "  timestep     |  720343\n",
            "  reward       |  0.427\n",
            "----------------------------------------\n",
            "INFO - Step 123100, rl-loss: 0.005242757499217987\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 124100, rl-loss: 0.0063792020082473755\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 125100, rl-loss: 0.014099153690040112\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 126100, rl-loss: 0.008160414174199104\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 127100, rl-loss: 0.015485244803130627\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 128100, rl-loss: 0.021444637328386307\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 129100, rl-loss: 0.010545497760176659\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 130100, rl-loss: 0.007970810867846012\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 131100, rl-loss: 0.00959454383701086\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 132100, rl-loss: 0.009589256718754768\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 133100, rl-loss: 0.0081869475543499\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 134100, rl-loss: 0.009891601279377937\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 135100, rl-loss: 0.009793903678655624\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 136100, rl-loss: 0.0167540255934\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 137100, rl-loss: 0.007993152365088463\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 137391, rl-loss: 0.009446829557418823\n",
            "----------------------------------------\n",
            "  timestep     |  810477\n",
            "  reward       |  0.439\n",
            "----------------------------------------\n",
            "INFO - Step 138100, rl-loss: 0.008525663986802101\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 139100, rl-loss: 0.006069489289075136\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 140100, rl-loss: 0.006573980674147606\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 141100, rl-loss: 0.008162936195731163\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 142100, rl-loss: 0.009833547286689281\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 143100, rl-loss: 0.008524253964424133\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 144100, rl-loss: 0.01127661019563675\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 145100, rl-loss: 0.009208216331899166\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 146100, rl-loss: 0.004058851860463619\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 147100, rl-loss: 0.0112250791862607\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 148100, rl-loss: 0.013799535110592842\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 149100, rl-loss: 0.004389951936900616\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 150100, rl-loss: 0.00945211946964264\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 151100, rl-loss: 0.009487954899668694\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 152100, rl-loss: 0.02583354525268078\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 152554, rl-loss: 0.007906734943389893\n",
            "----------------------------------------\n",
            "  timestep     |  900092\n",
            "  reward       |  0.446\n",
            "----------------------------------------\n",
            "INFO - Step 153100, rl-loss: 0.006986703723669052\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 154100, rl-loss: 0.015197545289993286\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 155100, rl-loss: 0.007705086842179298\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 156100, rl-loss: 0.009448658674955368\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 157100, rl-loss: 0.012472116388380527\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 158100, rl-loss: 0.010184984654188156\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 159100, rl-loss: 0.007408313453197479\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 160100, rl-loss: 0.019618134945631027\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 161100, rl-loss: 0.012280194088816643\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 162100, rl-loss: 0.0110593531280756\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 163100, rl-loss: 0.008227064274251461\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 164100, rl-loss: 0.011107890866696835\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 165100, rl-loss: 0.0041155703365802765\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 166100, rl-loss: 0.009276996366679668\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 167100, rl-loss: 0.007430524565279484\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 168026, rl-loss: 0.005069767590612173\n",
            "----------------------------------------\n",
            "  timestep     |  990673\n",
            "  reward       |  0.408\n",
            "----------------------------------------\n",
            "INFO - Step 168100, rl-loss: 0.008299014531075954\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 169100, rl-loss: 0.009662026539444923\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 170100, rl-loss: 0.013125220313668251\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 171100, rl-loss: 0.005478982347995043\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 172100, rl-loss: 0.020329708233475685\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 173100, rl-loss: 0.01275601051747799\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 174100, rl-loss: 0.013563304208219051\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 175100, rl-loss: 0.006541802082210779\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 176100, rl-loss: 0.025214143097400665\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 177100, rl-loss: 0.003405004972591996\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 178100, rl-loss: 0.003873142646625638\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 179100, rl-loss: 0.008200503885746002\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 180100, rl-loss: 0.009895279072225094\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 181100, rl-loss: 0.011042207479476929\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 182100, rl-loss: 0.006173266097903252\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 183100, rl-loss: 0.008230285719037056\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 183284, rl-loss: 0.00392439030110836\n",
            "----------------------------------------\n",
            "  timestep     |  1080091\n",
            "  reward       |  0.431\n",
            "----------------------------------------\n",
            "INFO - Step 184100, rl-loss: 0.00883319042623043\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 185100, rl-loss: 0.014261468313634396\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 186100, rl-loss: 0.008298177272081375\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 187100, rl-loss: 0.007012180518358946\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 188100, rl-loss: 0.00847579911351204\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 189100, rl-loss: 0.005553070455789566\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 190100, rl-loss: 0.020615968853235245\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 191100, rl-loss: 0.013636309653520584\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 192100, rl-loss: 0.005190624855458736\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 193100, rl-loss: 0.00474208127707243\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 194100, rl-loss: 0.011573972180485725\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 195100, rl-loss: 0.006519918330013752\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 196100, rl-loss: 0.006879215128719807\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 197100, rl-loss: 0.01390116661787033\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 198100, rl-loss: 0.00814567320048809\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 198732, rl-loss: 0.006814079359173775\n",
            "----------------------------------------\n",
            "  timestep     |  1169927\n",
            "  reward       |  0.43\n",
            "----------------------------------------\n",
            "INFO - Step 199100, rl-loss: 0.01360178180038929\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 200100, rl-loss: 0.012081325054168701\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 201100, rl-loss: 0.006205531768500805\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 202100, rl-loss: 0.02131027914583683\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 203100, rl-loss: 0.009234162047505379\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 204100, rl-loss: 0.0048595634289085865\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 205100, rl-loss: 0.005325170233845711\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 206100, rl-loss: 0.002757664769887924\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 207100, rl-loss: 0.015028214082121849\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 208100, rl-loss: 0.009705232456326485\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 209100, rl-loss: 0.007942939177155495\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 210100, rl-loss: 0.009226826950907707\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 211100, rl-loss: 0.005819863174110651\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 212100, rl-loss: 0.006952892988920212\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 213100, rl-loss: 0.006776169873774052\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 213949, rl-loss: 0.013420282863080502\n",
            "----------------------------------------\n",
            "  timestep     |  1259957\n",
            "  reward       |  0.441\n",
            "----------------------------------------\n",
            "INFO - Step 214100, rl-loss: 0.009309113025665283\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 215100, rl-loss: 0.015834679827094078\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 216100, rl-loss: 0.004484109114855528\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 217100, rl-loss: 0.004326837137341499\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 218100, rl-loss: 0.010674127377569675\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 219100, rl-loss: 0.016846809536218643\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 220100, rl-loss: 0.006896969862282276\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 221100, rl-loss: 0.005926134530454874\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 222100, rl-loss: 0.010443100705742836\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 223100, rl-loss: 0.00840455386787653\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 224100, rl-loss: 0.007205859757959843\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 225100, rl-loss: 0.008748872205615044\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 226100, rl-loss: 0.010946735739707947\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 227100, rl-loss: 0.004556805826723576\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 228100, rl-loss: 0.004178098868578672\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 229100, rl-loss: 0.008393438532948494\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 229136, rl-loss: 0.01210323441773653\n",
            "----------------------------------------\n",
            "  timestep     |  1349312\n",
            "  reward       |  0.457\n",
            "----------------------------------------\n",
            "INFO - Step 230100, rl-loss: 0.010965454392135143\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 231100, rl-loss: 0.008082985877990723\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 232100, rl-loss: 0.007762144785374403\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 233100, rl-loss: 0.007092647720128298\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 234100, rl-loss: 0.01464239601045847\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 235100, rl-loss: 0.00906981062144041\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 236100, rl-loss: 0.04116442799568176\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 237100, rl-loss: 0.0057710823602974415\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 238100, rl-loss: 0.006704200059175491\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 239100, rl-loss: 0.004426784813404083\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 240100, rl-loss: 0.012425124645233154\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 241100, rl-loss: 0.008256926201283932\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 242100, rl-loss: 0.009831523522734642\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 243100, rl-loss: 0.008820764720439911\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 244100, rl-loss: 0.009989341720938683\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 244376, rl-loss: 0.005732616875320673\n",
            "----------------------------------------\n",
            "  timestep     |  1439001\n",
            "  reward       |  0.412\n",
            "----------------------------------------\n",
            "INFO - Step 245100, rl-loss: 0.027282943949103355\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 246100, rl-loss: 0.01168719120323658\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 247100, rl-loss: 0.007877184078097343\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 248100, rl-loss: 0.0079130157828331\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 249100, rl-loss: 0.011591890826821327\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 250100, rl-loss: 0.008041897788643837\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 251100, rl-loss: 0.0072801196947693825\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 252100, rl-loss: 0.01988273672759533\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 253100, rl-loss: 0.0052682096138596535\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 254100, rl-loss: 0.00993326399475336\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 255100, rl-loss: 0.010906768962740898\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 256100, rl-loss: 0.005770230665802956\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 257100, rl-loss: 0.004681895487010479\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 258100, rl-loss: 0.007520848885178566\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 259100, rl-loss: 0.00502663291990757\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 259714, rl-loss: 0.01236619520932436\n",
            "----------------------------------------\n",
            "  timestep     |  1530209\n",
            "  reward       |  0.454\n",
            "----------------------------------------\n",
            "INFO - Step 260100, rl-loss: 0.006398427300155163\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 261100, rl-loss: 0.010589415207505226\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 262100, rl-loss: 0.016239985823631287\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 263100, rl-loss: 0.012038405984640121\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 264100, rl-loss: 0.005131860729306936\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 265100, rl-loss: 0.019807137548923492\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 266100, rl-loss: 0.01765904203057289\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 266442, rl-loss: 0.006085647270083427\n",
            "Logs saved in experiments/simple_proba_vs_self_0.1_0.8_256x3_thr6\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-bb0844e4d4ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_agents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"CUDA_VISIBLE_DEVICES\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mevaluate_vs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSimpleAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_actions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-5-859798256337>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(args, env, agent, evaluate_vs)\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0;31m# and the other players play randomly (if any)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mts\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrajectories\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m                 \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopponent\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m\"self\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/rlcard/agents/dqn_agent.py\u001b[0m in \u001b[0;36mfeed\u001b[0;34m(self, ts)\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0mtmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_t\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplay_memory_init_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtmp\u001b[0m\u001b[0;34m>=\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtmp\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_every\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/rlcard/agents/dqn_agent.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    212\u001b[0m         \u001b[0mstate_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\rINFO - Step {}, rl-loss: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/rlcard/agents/dqn_agent.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, s, a, y)\u001b[0m\n\u001b[1;32m    329\u001b[0m         \u001b[0mbatch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0mbatch_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m         \u001b[0mbatch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    169\u001b[0m                  \u001b[0mmaximize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'maximize'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m                  \u001b[0mforeach\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'foreach'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m                  capturable=group['capturable'])\n\u001b[0m\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    224\u001b[0m          \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m          \u001b[0mmaximize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmaximize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m          capturable=capturable)\n\u001b[0m\u001b[1;32m    227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable)\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m         \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m         \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcapturable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate - TODO"
      ],
      "metadata": {
        "id": "gzNtmxV-hWP9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 durak_rlcard/evaluate.py --models /content/drive/MyDrive/Github/durak-project/durak_rlcard/experiments_10000_vs_random/model.pth random --cuda=0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jCl09eqy8MC6",
        "outputId": "53139fe8-b8f7-46aa-df7b-f03c57b74b96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--> Running on the GPU\n",
            "0 /content/drive/MyDrive/Github/durak-project/durak_rlcard/experiments_10000_vs_random/model.pth 0.348\n",
            "1 random 0.652\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 durak_rlcard/evaluate.py --models /content/drive/MyDrive/Github/durak-project/durak_rlcard/experiments_10000_vs_random/model.pth simple --cuda=0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ThUM9CEI-REZ",
        "outputId": "b4671add-b389-43b7-e44e-fec0d19a2327"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--> Running on the GPU\n",
            "0 /content/drive/MyDrive/Github/durak-project/durak_rlcard/experiments_10000_vs_random/model.pth 0.0145\n",
            "1 simple 0.9855\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#/content/drive/MyDrive/Github/durak-project/experiments/simple_proba_vs_self_0.1_0.5_64x3_thr0/model.pth\n",
        "!python3 durak_rlcard/evaluate.py --models /content/drive/MyDrive/Github/durak-project/experiments/simple_proba_vs_self_0.1_0.5_64x3_thr0/model.pth random --cuda=0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_1VoLqRt-e9G",
        "outputId": "fc7e0f1d-9e63-4466-bc84-d1f8ed4c7f4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--> Running on the GPU\n",
            "0 /content/drive/MyDrive/Github/durak-project/experiments/simple_proba_vs_self_0.1_0.5_64x3_thr0/model.pth 0.9795\n",
            "1 random 0.0205\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#/content/drive/MyDrive/Github/durak-project/experiments/simple_proba_vs_self/model.pth\n",
        "!python3 durak_rlcard/evaluate.py --models /content/drive/MyDrive/Github/durak-project/experiments/simple_proba_vs_self/model.pth random --cuda=0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6cxyoouu-zOS",
        "outputId": "4171f73a-3162-4e62-d99d-a9196cbc56e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--> Running on the GPU\n",
            "0 /content/drive/MyDrive/Github/durak-project/experiments/simple_proba_vs_self/model.pth 0.759\n",
            "1 random 0.241\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#/content/drive/MyDrive/Github/durak-project/experiments/simple_proba_vs_self/model.pth\n",
        "!python3 durak_rlcard/evaluate.py --models /content/drive/MyDrive/Github/durak-project/experiments/simple_proba_vs_self/model.pth simple --cuda=0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LhTZMD4G_INK",
        "outputId": "e3c16661-8373-4035-d828-c0cd8dd8a609"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--> Running on the GPU\n",
            "0 /content/drive/MyDrive/Github/durak-project/experiments/simple_proba_vs_self/model.pth 0.074\n",
            "1 simple 0.926\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#/content/drive/MyDrive/Github/durak-project/experiments/simple_proba_vs_self_threshold_0.1_0.6_thr_6/model.pth\n",
        "!python3 durak_rlcard/evaluate.py --models /content/drive/MyDrive/Github/durak-project/experiments/simple_proba_vs_self_threshold_0.1_0.6_thr_6/model.pth random --cuda=0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aItOeESq_0uy",
        "outputId": "57ed68fb-499c-476c-c14f-7edcf85cb79b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--> Running on the GPU\n",
            "0 /content/drive/MyDrive/Github/durak-project/experiments/simple_proba_vs_self_threshold_0.1_0.6_thr_6/model.pth 0.8985\n",
            "1 random 0.1015\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train NFSP on gpu"
      ],
      "metadata": {
        "id": "6hktZgpTftP2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 durak_rlcard/dqn.py --algorithm=nfsp --cuda=0"
      ],
      "metadata": {
        "id": "fx6gqWfhfsS9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train DMC on gpu"
      ],
      "metadata": {
        "id": "y99dJe5vf1HI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 durak_rlcard/dmc.py --cuda=0"
      ],
      "metadata": {
        "id": "d_aopPBB1_J3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 durak_rlcard/evaluate.py --cuda=0"
      ],
      "metadata": {
        "id": "cIGijRp4hVFd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vY3i8RbPgXFf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}